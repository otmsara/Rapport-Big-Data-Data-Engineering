```markdown
# Pipeline Big Data : Analyse de logs syst√®me en temps r√©el

## Description du projet
Ce projet est r√©alis√© dans le cadre du module **Big Data / Data Engineering**.  
L‚Äôobjectif est de concevoir un **pipeline Big Data complet** pour l‚Äôanalyse de logs syst√®me en temps r√©el, en utilisant les technologies suivantes :

- **Apache Kafka** : ingestion et transport des logs
- **Apache Spark (RDD, DataFrame, Streaming, SQL)** : traitement distribu√© et analyse
- **HDFS** : stockage distribu√© des logs trait√©s

Le pipeline simule la production de logs, leur traitement en streaming, leur stockage et leur analyse avec des requ√™tes SQL distribu√©es.

---

## Architecture du pipeline

```

Kafka Producer --> Kafka Topic --> Spark Streaming --> HDFS --> Spark SQL / Analyse

```

**√âtapes principales :**
1. Les logs sont g√©n√©r√©s et envoy√©s dans un **topic Kafka** (`logs-topic`).  
2. **Spark Streaming** consomme les messages Kafka et les √©crit dans **HDFS**.  
3. Les donn√©es stock√©es sont analys√©es avec **RDD, DataFrame et Spark SQL** pour obtenir des statistiques et identifier les erreurs critiques.

üìå *Remarque* : Les logs peuvent √™tre simul√©s en temps r√©el via le terminal Kafka Producer.

---

## Technologies utilis√©es

- **Kafka** : Producer, Topic, Consumer
- **Spark** : RDD, DataFrame, Spark Streaming, Spark SQL
- **HDFS** : Stockage distribu√©
- **Scala** : Langage de programmation principal
- **Windows 10** avec Java 1.8+ pour l‚Äôenvironnement local

---

## Contenu du d√©p√¥t

Le d√©p√¥t contient les √©l√©ments suivants :

```

/LogStreamingApp
‚îú‚îÄ build.sbt                  # Fichier de configuration SBT
‚îú‚îÄ project/                   # Configuration du projet Scala
‚îú‚îÄ src/main/scala/            # Code source Scala
‚îÇ   ‚îî‚îÄ com/example/logstreaming/
‚îÇ       ‚îî‚îÄ LogStreaming.scala # Code principal Spark Streaming
‚îú‚îÄ README.md                  # Ce fichier explicatif
‚îú‚îÄ rapport.pdf / rapport.docx  # Rapport complet du projet
‚îî‚îÄ PPTX                        # Pr√©sentation du pipeline

````

---

## Instructions pour ex√©cuter le projet

### Pr√©requis

- Java 1.8+  
- Apache Spark 3.x  
- Apache Kafka 2.x  
- Hadoop / HDFS 3.x  

### √âtapes de lancement

1. **D√©marrer Hadoop HDFS**  
```bat
cd C:\hadoop\sbin
start-dfs.cmd
````

2. **D√©marrer Zookeeper**

```bat
cd C:\kafka
bin\windows\zookeeper-server-start.bat config\zookeeper.properties
```

3. **D√©marrer Kafka Broker**

```bat
cd C:\kafka
bin\windows\kafka-server-start.bat config\server.properties
```

4. **Cr√©er le topic Kafka**

```bat
bin\windows\kafka-topics.bat --create --topic logs-topic --bootstrap-server localhost:9092
```

5. **Envoyer des logs via Kafka Producer**

```bat
bin\windows\kafka-console-producer.bat --topic logs-topic --bootstrap-server localhost:9092
```

6. **Lancer Spark Streaming (code Scala)**

```bat
cd C:\Users\HP\Desktop\LogStreamingApp
sbt "runMain com.example.logstreaming.LogStreaming"
```

7. **Analyser les donn√©es dans Spark Shell (RDD, DataFrame, SQL)**

```scala
val logsRDD = sc.textFile("hdfs://localhost:9000/logs/output/*")
val logsDF = spark.read.text("hdfs://localhost:9000/logs/output/*")
logsDF.createOrReplaceTempView("logs")
spark.sql("SELECT level, COUNT(*) FROM logs GROUP BY level").show()
```

---

## Livrables du projet

1. **Code source complet** (Scala / Spark)
2. **Rapport** (PDF / DOCX) : explication du pipeline, architecture, r√©sultats, captures d‚Äô√©cran
3. **PPTX** : pr√©sentation synth√©tique pour l‚Äôoral

---

## Remarques

* Les technologies utilis√©es peuvent √™tre nouvelles pour l‚Äô√©tudiant, mais elles sont essentielles pour illustrer un pipeline Big Data complet.
* Les donn√©es g√©n√©r√©es sont simul√©es pour des fins p√©dagogiques.
* Le projet est con√ßu pour fonctionner en local mais reste extensible √† un environnement distribu√© r√©el.

---

## Auteur

**Nom :** Sara El-otmani 6 Kenza El hariri
**Module :** Big Data / Data Engineering
**Encadrant :** Hassan BADIR
**Ann√©e :** 2025‚Äì2026
